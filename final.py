# ===================================================================
# AI í† ë§ˆí†  ìë™ ìˆ˜í™• ë¡œë´‡ ì‹œìŠ¤í…œ - v4 (ì¹´ë©”ë¼ ì•ˆì •í™” ê¸°ëŠ¥ ì¶”ê°€)
# ===================================================================
# - ì¹´ë©”ë¼ ìº¡ì²˜ ì‹œ ì•ˆì •í™” ì‹œê°„ì„ ë¶€ì—¬í•˜ê³  í•´ìƒë„ë¥¼ ê³ ì •í•˜ì—¬,
#   ë§¤ë²ˆ ì¼ê´€ë˜ê³  ì„ ëª…í•œ ì´ë¯¸ì§€ë¥¼ ì–»ë„ë¡ ìº¡ì²˜ ë¡œì§ì„ ê°œì„ í•©ë‹ˆë‹¤.
# ===================================================================

import torch
import cv2
import serial
import time
import os
import sys

# ì„¤ì •
YOLO_REPO_PATH = '/home/tomato/yolov5' # âš ï¸ ì¤‘ìš”: git cloneí•œ yolov5 í´ë” ê²½ë¡œ
MODEL_PATH = '/home/tomato/deeplearning/yolov5/best.pt'

# ì•„ë‘ì´ë…¸ ì‹œë¦¬ì–¼ í¬íŠ¸
SERIAL_PORT = '/dev/ttyACM0' # ë¼ì¦ˆë² ë¦¬íŒŒì´ì— ì—°ê²°ëœ ì•„ë‘ì´ë…¸ í¬íŠ¸ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.
# ì‹œë¦¬ì–¼ í†µì‹  ì†ë„
BAUD_RATE = 115200 # ì•„ë‘ì´ë…¸ ìŠ¤ì¼€ì¹˜ì™€ ë™ì¼í•œ ì†ë„ë¡œ ì„¤ì •

# --- ê³ ê¸‰ ì„¤ì • ---
CAM_INDEX = 0
FRAME_WIDTH = 640  # ìº¡ì²˜í•  í”„ë ˆì„ ë„ˆë¹„
FRAME_HEIGHT = 480 # ìº¡ì²˜í•  í”„ë ˆì„ ë†’ì´
CONF_THRESHOLD = 0.9
IOU_THRESHOLD = 0.4
RESULT_IMAGE_PATH = "detection_result.png"

# ì‹œë¦¬ì–¼ í†µì‹  ì‹ í˜¸ ì •ì˜
ARDUINO_CHECK_SIGNAL = "CHECK"
ARDUINO_END_SIGNAL = "END"
NO_TOMATO_SIGNAL = "None"
# -------------------------------------------------------------------

def initialize():
    """ëª¨ë¸, ì‹œë¦¬ì–¼ í¬íŠ¸ë¥¼ ëª¨ë‘ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    print("--- ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘ ---")
    model = None
    print("YOLOv5 ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...")
    try:
        model = torch.hub.load(YOLO_REPO_PATH, 'custom', path=MODEL_PATH, source='local')
        model.conf = CONF_THRESHOLD
        model.iou = IOU_THRESHOLD
        print("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
    except Exception as e:
        print(f"ì˜¤ë¥˜: YOLO ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. {e}")
        sys.exit()

    ser = None
    try:
        ser = serial.Serial(SERIAL_PORT, BAUD_RATE, timeout=None)
        time.sleep(2)
        print(f"ì•„ë‘ì´ë…¸({SERIAL_PORT}) ì—°ê²° ì™„ë£Œ.")
    except serial.SerialException as e:
        print(f"âŒ ì˜¤ë¥˜: ì‹œë¦¬ì–¼ í¬íŠ¸ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. {e}")
        sys.exit()
    
    print("--- ì´ˆê¸°í™” ì™„ë£Œ ---")
    return model, ser

def preprocess_frame(frame):
    """
    ìº¡ì²˜ëœ í”„ë ˆì„ì˜ í’ˆì§ˆì„ í–¥ìƒì‹œì¼œ íƒì§€ìœ¨ì„ ë†’ì…ë‹ˆë‹¤.
    1. ìë™ ëŒ€ë¹„ ì¡°ì ˆ (CLAHE) - ë” ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€ë¹„ í–¥ìƒ
    2. ë…¸ì´ì¦ˆ ì œê±° (Bilateral Filter) - ê²½ê³„ì„ ì€ ìœ ì§€í•˜ë©° ë…¸ì´ì¦ˆ ì œê±°
    """
    # 1. ì»¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ LAB ìƒ‰ ê³µê°„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ L(ë°ê¸°) ì±„ë„ì—ë§Œ ì ìš©
    lab = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)
    l, a, b = cv2.split(lab)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    cl = clahe.apply(l)
    limg = cv2.merge((cl,a,b))
    equalized_frame = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)
    
    # 2. ë…¸ì´ì¦ˆ ì œê±°
    denoised_frame = cv2.bilateralFilter(equalized_frame, 9, 75, 75)
    
    return denoised_frame

def capture_and_detect(model):
    """[âœ¨ ê°œì„  ì‚¬í•­] ì•ˆì •í™”ëœ ì¹´ë©”ë¼ í”„ë ˆì„ì„ ìº¡ì²˜í•˜ê³ , ì „ì²˜ë¦¬ í›„, ê°ì²´ë¥¼ íƒì§€í•©ë‹ˆë‹¤."""
    cap = cv2.VideoCapture(CAM_INDEX)
    if not cap.isOpened():
        print(f"ì˜¤ë¥˜: ì¹´ë©”ë¼(ì¸ë±ìŠ¤: {CAM_INDEX})ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None, None
    
    try:
        # [âœ¨ ê°œì„  ì‚¬í•­ 1] í•´ìƒë„ ëª…ì‹œì  ì„¤ì •
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, FRAME_WIDTH)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, FRAME_HEIGHT)
        print(f"ì¹´ë©”ë¼ í•´ìƒë„ë¥¼ {FRAME_WIDTH}x{FRAME_HEIGHT}ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
        
        # [âœ¨ ê°œì„  ì‚¬í•­ 2] ì¹´ë©”ë¼ ì•ˆì •í™” ì‹œê°„ ë¶€ì—¬
        # ìë™ ë…¸ì¶œ, ì´ˆì  ë“±ì´ ì•ˆì •ë  ì‹œê°„ì„ ì¤ë‹ˆë‹¤.
        time.sleep(1) 

        # [âœ¨ ê°œì„  ì‚¬í•­ 3] ë²„í¼ í´ë¦¬ì–´ë§
        # ë¶ˆì•ˆì •í•œ ì´ˆê¸° í”„ë ˆì„ì„ ë²„ë¦¬ê¸° ìœ„í•´ ì—¬ëŸ¬ ë²ˆ ì½ì–´ë“¤ì…ë‹ˆë‹¤.
        for _ in range(10):
            ret, frame = cap.read()
            if not ret:
                print("ì˜¤ë¥˜: ì¹´ë©”ë¼ ì•ˆì •í™” ì¤‘ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None, None
        
        print("âœ… ì¹´ë©”ë¼ ì•ˆì •í™” ë° ìµœì¢… í”„ë ˆì„ ìº¡ì²˜ ì™„ë£Œ.")

    finally:
        cap.release() # ì‘ì—…ì´ ëë‚˜ë©´ ë°˜ë“œì‹œ ì¹´ë©”ë¼ ë¦¬ì†ŒìŠ¤ë¥¼ í•´ì œí•©ë‹ˆë‹¤.
    
    if not ret or frame is None:
        print("ì˜¤ë¥˜: ìµœì¢… í”„ë ˆì„ ìº¡ì²˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return None, None
    
    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë‹¨ê³„
    print("ì´ë¯¸ì§€ë¥¼ ì „ì²˜ë¦¬í•˜ì—¬ í’ˆì§ˆì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤...")
    preprocessed_frame = preprocess_frame(frame)

    print("ì´ë¯¸ì§€ì—ì„œ ê°ì²´ íƒì§€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤...")
    results = model(preprocessed_frame)
    
    return frame, results # ì‹œê°í™”ëŠ” ì›ë³¸ í”„ë ˆì„(frame)ì— í•˜ê¸° ìœ„í•´ í•¨ê»˜ ë°˜í™˜

# process_results, perform_scan_and_send, main í•¨ìˆ˜ëŠ” ì´ì „ê³¼ ë™ì¼í•©ë‹ˆë‹¤.
def process_results(frame, results):
    """íƒì§€ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ì›ë³¸ í”„ë ˆì„ì— ì‹œê°í™”í•˜ë©° ì¢Œí‘œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    detections = results.xyxy[0]
    coords_to_send = []
    
    colors = {0: (0, 255, 255), 1: (0, 165, 255), 2: (0, 0, 255), 3: (255, 0, 255)}

    for *box, conf, cls_idx_tensor in detections:
        x1, y1, x2, y2 = map(int, box)
        cls_idx = int(cls_idx_tensor)
        
        if cls_idx not in colors: continue
            
        xc = (x1 + x2) // 2
        yc = (y1 + y2) // 2
        color = colors[cls_idx]
        
        print(f"  â†’ (ì‹ ë¢°ë„: {conf:.2f}), ì¤‘ì‹¬: ({xc}, {yc})")
        coords_to_send.append(f"{xc},{yc}")

        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)

    cv2.imwrite(RESULT_IMAGE_PATH, frame)
    print(f"ğŸ–¼íƒì§€ ê²°ê³¼ê°€ '{RESULT_IMAGE_PATH}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return coords_to_send

def perform_scan_and_send(model, ser):
    """ìŠ¤ìº”, íƒì§€, ê²°ê³¼ ë¶„ì„, ë°ì´í„° ì „ì†¡ê¹Œì§€ì˜ í•œ ì‚¬ì´í´ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    original_frame, results = capture_and_detect(model)
    if original_frame is None or results is None:
        print("ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜¤ë¥˜. ì‘ì—…ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    coords_to_send = process_results(original_frame, results)

    if coords_to_send:
        data_str = '/'.join(coords_to_send) + '/\n'
        ser.write(data_str.encode('utf-8'))
        print(f"ì¢Œí‘œ ì „ì†¡: {data_str.strip()}")
    else:
        data_str = NO_TOMATO_SIGNAL + '\n'
        ser.write(data_str.encode('utf-8'))
        print(f"íƒì§€ëœ í† ë§ˆí†  ì—†ìŒ: '{NO_TOMATO_SIGNAL}' ì‹ í˜¸ ì „ì†¡")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ì²« ìŠ¤ìº”ì€ ìë™, ì´í›„ëŠ” ìš”ì²­-ì‘ë‹µ ë°©ì‹)"""
    model, ser = initialize()

    try:
        print("\ní”„ë¡œê·¸ë¨ ì‹œì‘! ì²« ë²ˆì§¸ ìŠ¤ìº”ì„ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
        perform_scan_and_send(model, ser)

        while True:
            print(f"\nì•„ë‘ì´ë…¸ë¡œë¶€í„° '{ARDUINO_CHECK_SIGNAL}' ë˜ëŠ” '{ARDUINO_END_SIGNAL}' ì‹ í˜¸ ìˆ˜ì‹  ëŒ€ê¸° ì¤‘...")
            line = ser.readline().decode('utf-8').strip()

            if line == ARDUINO_CHECK_SIGNAL:
                print(f"âœ… '{line}' ì‹ í˜¸ ìˆ˜ì‹ ! ë‹¤ìŒ ìŠ¤ìº”ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
                perform_scan_and_send(model, ser)

            elif line == ARDUINO_END_SIGNAL:
                print(f"âœ… '{line}' ì‹ í˜¸ ìˆ˜ì‹ ! í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            else:
                if line:
                    print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì‹ í˜¸ ìˆ˜ì‹ : '{line}'. ë¬´ì‹œí•˜ê³  ëŒ€ê¸°í•©ë‹ˆë‹¤.")

    except KeyboardInterrupt:
        print("\nì‚¬ìš©ìì— ì˜í•´ í”„ë¡œê·¸ë¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        if ser and ser.is_open:
            ser.close()
            print("ì‹œë¦¬ì–¼ í¬íŠ¸ ì—°ê²°ì„ ë‹«ì•˜ìŠµë‹ˆë‹¤.")
        print("--- í”„ë¡œê·¸ë¨ ì¢…ë£Œ ---")

if __name__ == '__main__':
    main()



